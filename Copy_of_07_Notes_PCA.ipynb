{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bharatkaushik2015/data-processing-interview-questions/blob/main/Copy_of_07_Notes_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content"
      ],
      "metadata": {
        "id": "o4Gc61YSizTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Motivation for High dim viz\n",
        "\n",
        "- PCA Introduction/ Intuition\n",
        "\n",
        "- Applying PCA\n",
        "\n",
        "- Geometrical Objective\n",
        "\n",
        "- Mathematical Proof\n",
        "\n",
        "- Eigen values and vectors\n",
        "\n"
      ],
      "metadata": {
        "id": "wlpReNN3eB43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What we have learnt so far? Motivation for High dim viz"
      ],
      "metadata": {
        "id": "rhC2AM_SiT8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We started off with clustering followed by anomaly detection\n",
        "\n",
        "In clustering,\n",
        "- the main question was **number of clusters**\n",
        "\n",
        "where as in anomaly detection\n",
        "- the question was how many outlier do we feel there should be\n",
        "- and we handled this using **contamination** parameter."
      ],
      "metadata": {
        "id": "P0p082V5ig7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How did we tackle these questions ?"
      ],
      "metadata": {
        "id": "B2a-oxxflTmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to answer the question of **number of clusters**\n",
        "- there were technique which such as\n",
        "    - **elbow method** in Kmeans\n",
        "    - **Dendrogram** in Hierarichal etc\n",
        "\n",
        "\n",
        "But these were just the guidelines.\n",
        "\n"
      ],
      "metadata": {
        "id": "wSa2RANEkvO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Are these guidelines enough to make the decision ?"
      ],
      "metadata": {
        "id": "l8x1PyYBnNMH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **primary way** we decide/ come to the conclusion\n",
        "- is by **validating it with the business**\n",
        "\n",
        "\n",
        "For example: Recall the Kmeans cluster of discount shoppers, new user etc.\n",
        "\n",
        "We made sure the clusters we arrived at, were **making business sense**."
      ],
      "metadata": {
        "id": "fX8Rwr_7lQs1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need some help from the visualization in order to make those decision i.e.\n",
        "- as elbow method alone is not suffice\n",
        "\n",
        "So, there is a need for visualizing the data."
      ],
      "metadata": {
        "id": "rB1XYp1lmBKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What data did we work on so far ?"
      ],
      "metadata": {
        "id": "tFpQAHa4m244"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we have only worked with simple data\n",
        "\n",
        "Recall\n",
        "- We took 2D data for anomaly detection (Cars 24)\n",
        "- and 5D data for clustering and visualized it in 3D\n",
        "    - or we made a polar chart to help us\n",
        "\n",
        "But the real world data\n",
        "- can have 20 dimension or more.\n",
        "\n",
        "**How do we visualize this high dim data?**\n",
        "\n",
        "For this, we'll be studying high dim viz techniques\n",
        "- which will help us in visualizing high dim data as a shadow in small dim space.\n",
        "- and make a more informed guess\n",
        "\n",
        "Let's into it"
      ],
      "metadata": {
        "id": "LTpSPhdxnLw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principal Component Analysis"
      ],
      "metadata": {
        "id": "A2_U2GpIomjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction"
      ],
      "metadata": {
        "id": "9GS-HtXZpkN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Say, we are given data of patient's weight and age\n",
        "- and we want to predict whether person is diabetic or not\n",
        "\n",
        "\n",
        "Say, the scatter plot of the data is as follows:"
      ],
      "metadata": {
        "id": "bV5D_3UEp3kb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1BNTFYHMtyDRsag1N9x7TQFLHbvEQXPI8\" width=800></center>"
      ],
      "metadata": {
        "id": "ZBjs9AssCCzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Which feature has more information ? Age or weight?**\n",
        "\n",
        "\n",
        "\n",
        "> Note: Here, by information, we mean w.r.t solving the classification problem"
      ],
      "metadata": {
        "id": "jlba8BgsCOqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, weight carries more information\n",
        "- as most of the people have roughly the same age.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ifUcsAxlCoSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, if we imagine this as a classifcation problem\n",
        "- there's a chance that people with higher weight belongs to diabetes class\n",
        "- while ones with lower weight belong to non diabetic class"
      ],
      "metadata": {
        "id": "E3MMGohVEuyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://drive.google.com/uc?export=view&id=19TLkPz7x6qKkRfg4gBRZ3GJI14lSd2U2\" width=600></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "8XjRrEl_D31B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To conclude:\n",
        "- weight is the feature which is helping us distinguish the classes.\n",
        "\n",
        "That means, for this dataset\n",
        "- Although we have two features,\n",
        "    - maybe one of them is important (weight) while the other (age) is not that important.\n",
        "\n",
        "**But why weight feature ?**\n",
        "- as age is more or less constant\n",
        "- and most of the variance lies along the weight"
      ],
      "metadata": {
        "id": "x08nQ8-HEpn9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Say, we change the features now\n",
        "\n",
        "- and bring in weight and height."
      ],
      "metadata": {
        "id": "AhZrw00sE6Sx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1T8qOis-oVmvqh-ICiq9q2hNUuJ14d1sF\" width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "-tX8GXIcFcMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Which feature will be more important now ?**\n",
        "\n",
        "a. Weight\n",
        "\n",
        "b. Height\n",
        "\n",
        "c. New feature vector along Weight + Height"
      ],
      "metadata": {
        "id": "rPXUGUlcHHCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the information will lie along the vector made with combination of height and weight.\n",
        "\n",
        "The direction of the vector will be:"
      ],
      "metadata": {
        "id": "x6p68lV1HUsA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=11iKGHiUjgN7IVhbXEyl0JCcBlOFqC8YF\" width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "JFFj2THaMmpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a clear picture, let's do a axis transformation along the new vector"
      ],
      "metadata": {
        "id": "k-R9BPe3NDiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1TQGBFY_42S_1HzEVySB76aCkv-kxAeQ3\" width=800></center>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "98Bnko1WNc0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that,\n",
        "- maximum variance of data lies along the vector w+h"
      ],
      "metadata": {
        "id": "e1DSDJlvOWdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observation"
      ],
      "metadata": {
        "id": "0uDdXC9qOkpF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Not every feature contains equal info.\n",
        "    - it may not help in solving the task at hand\n",
        "    - i.e feature may carry noise/ diluted information.\n",
        "\n",
        "- a combination of new feature may have dense information\n",
        "    - i.e. it is rich in info compared to original feature\n",
        "\n"
      ],
      "metadata": {
        "id": "9sQExMx6PecV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on observation,\n",
        "- we can take the original feature space and\n",
        "- **transform** it into new smaller feature space i.e. **less number of features**\n",
        "\n",
        "such that\n",
        "- each of these new features contain information more densely\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5KelLkXWRS6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1KwXjq44SmIqpRf1vg_xfPX15f5sy7kkd\" width=800></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DXcOjzaURoAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the original feature space had 100% of info/variance\n",
        "\n",
        "- and new transformed feature space are very less in number compared to original one\n",
        "- but retain 90% of variance/ information.\n",
        "\n",
        "\n",
        "#### Conclusion:\n",
        "\n",
        "\n",
        "Against a small trade off of information loss,\n",
        "- we are able to reduce the number of features.\n",
        "\n",
        "- These new features are called **principal components**\n"
      ],
      "metadata": {
        "id": "9dlSNWLbeY0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, in our previous example, the principal components will be:"
      ],
      "metadata": {
        "id": "s9gpE38Al9WS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1GmpGoNrVv3KMEtcupERNlt3vqGcoZ3N_\" width=800></center>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aYpdQlhcmcVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interesting properties of these new transformed features/ principal components is:\n",
        "\n",
        "- they are perpendicular/ orthogonal to each other\n",
        "\n",
        "- The number of PC will be equal to the number of features present\n",
        "    - i.e. if we had 20 features, there will be 20 new transformed features/ PC.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6lzVAhETmytV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of these 20 PC, it is possible that\n",
        "- 10 features are able to explain 90% of variance/ info\n",
        "- while rest of the 10 features explain 10% of variance info collectively\n",
        "\n",
        "So, we can decide to keep 10 features\n",
        "- and hence reducing the features needed."
      ],
      "metadata": {
        "id": "SJXJk1h1nVyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=15Hj38Z0xp-GGOlMUZYlPEL1CbIr-32gT\" width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "IOqvta77opfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### But, why are we reducing number of features ?"
      ],
      "metadata": {
        "id": "GObI2eSle4Ec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Visualization"
      ],
      "metadata": {
        "id": "yPGlbwnAfulp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In order to visualize the data,\n",
        "    - we need to bring down the dim to 2 or 3.\n",
        "\n",
        "So that\n",
        "- we can make business decision based on the visualized data.\n",
        "\n",
        "Another need to for visualization is during the process of EDA\n"
      ],
      "metadata": {
        "id": "B6AJ4UJhfx83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####  Compression\n"
      ],
      "metadata": {
        "id": "Sn3Jo_OKf_43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Imagine you are training a DT with 100 features.\n",
        "- After applying PCA,\n",
        "    - you were able to compress feature space to 25 features.\n",
        "\n",
        "Now, using these new 25 features,\n",
        "- we can train our DT model\n",
        "\n",
        "This ultimately leads to **faster model training**."
      ],
      "metadata": {
        "id": "XM_bqhJHgvTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying PCA\n"
      ],
      "metadata": {
        "id": "Oz8zjXQ9g2Et"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Case 1:"
      ],
      "metadata": {
        "id": "009060Mk8DYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Say, we have the following data in 3 dimensions\n",
        "\n",
        "where the plane represents the decision boundary."
      ],
      "metadata": {
        "id": "9duEcPg5ri1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1WJCDchbbXQHWToWH6Qk-N_Ehi06RdknI\" width=500></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zQCd6Cuu0LOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we apply PCA on these given features (x, y, z),\n",
        "- we'll get 3 Principal Components (PC)\n",
        "\n",
        "These PCs will be:"
      ],
      "metadata": {
        "id": "28E2D4bB0LkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1WXvR_jz_55GzQ1ZN2UGRdgUVLZ3BbIZ7\" width=500></center>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Tt4dvZ0E1pdi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of these 3 PC,\n",
        "- PC1 is able to capture most of the variance\n",
        "- while rest of the two don't preserve much of info.\n",
        "\n",
        "So, we can discard PC2 and PC3"
      ],
      "metadata": {
        "id": "soK2d7fd3nix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this PC1,\n",
        "- we'll convert given 3 D data into 1D"
      ],
      "metadata": {
        "id": "BrB8k7KW395h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How will we do so?"
      ],
      "metadata": {
        "id": "BSZXAtGX8N6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to do so,\n",
        "- we need to find the coordinates of points on the PC1."
      ],
      "metadata": {
        "id": "L66vYbCS8PgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Which of the following is the correct way to get the coordinates ?"
      ],
      "metadata": {
        "id": "-h6Drcxb8c9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=12yush9rYpyaKq4DlqBqwsR7qHq7Gz8la\" width=800></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WMwlTTYNAEMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to get the coordinates of a point along a line/plane,\n",
        "- we take the projection of point on the line/plane.\n",
        "\n",
        "Projection is taken perpendicular to plane/line\n",
        "\n",
        "So, option 2 is correct."
      ],
      "metadata": {
        "id": "3vXOaTGxAxSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we take take the projection of points on a line,\n",
        "\n",
        "we'll get something like this:"
      ],
      "metadata": {
        "id": "eHDZOHAzBHYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1BdTdMp3pjmELrq3W0qQaHJit7nFipPsM\" width=800></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XVy40a7zBTNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By taking the projection of points on the Principal axis,\n",
        "- we have converted our 3D data to 1D data"
      ],
      "metadata": {
        "id": "A0NEdlYUbt3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before applying PCA,\n",
        "- we had 100% accuracy in classifying <font color=\"red\">red points</font> from <font color=\"green\">green points</font>\n",
        "\n",
        "\n",
        "After converting the data to 1D,\n",
        "- are still able to classify both the classes easily.\n",
        "\n",
        "\n",
        "So, the idea is\n",
        "- we don't need 3 features\n",
        "- instead of 3, we can create a new feature which is more efficicent\n",
        "- and we can use that new feature instead of 3."
      ],
      "metadata": {
        "id": "1YebUz5JbbOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, in new feature space,\n",
        "\n",
        "- the decision boundary will be a point\n"
      ],
      "metadata": {
        "id": "vBG9XuXnb45r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1FRNSJNLVkvL4G8JO9ifLXwqkL_3AXcal\" width=800></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6WX8GP93cYIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **aim of PCA** is\n",
        "- to preserve as much information as we can in less number of features."
      ],
      "metadata": {
        "id": "AqgIHWYKc62z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Case 2"
      ],
      "metadata": {
        "id": "GWG0vaRAdXur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Say, we have the following data:"
      ],
      "metadata": {
        "id": "tWM2JEuweV7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=102mM8flwjZxjGII_9cYoBCTWR8OetQiw\" width=600></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "2oTMs79leYCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to find the Principal component\n",
        "- which will preseve maximum variance of data\n",
        "\n",
        "The variance can be preseved by only two of the PCs as follows:"
      ],
      "metadata": {
        "id": "40yOCImYea_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1HS9dzqCJOlzaAkPRu_m34p0C_gZCnqz-\" width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "t4fZtv7keza-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we project the data onto this plane,\n",
        "\n",
        "the data will look something like this:"
      ],
      "metadata": {
        "id": "Yu12I8JPg9NR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1NV-CuxarujjgSVHYzEXXVFiIaA5vE9b-\" width=800></center>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dqik-EiihJgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Conclusion:"
      ],
      "metadata": {
        "id": "dTX_qpighMYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA can be referred as :\n",
        "- act of finding new axis to represent the data such that\n",
        "    - most of the information can be preseved by less number of principal components\n",
        "\n",
        "However, this may lead to some loss in information.\n",
        "\n",
        "Let's see how this loss of info happens"
      ],
      "metadata": {
        "id": "LieTfl2ehglj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Case 3:"
      ],
      "metadata": {
        "id": "Fd2RVMEoiy8L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Which of the following is better pick for first principal component?"
      ],
      "metadata": {
        "id": "gOk_YXnRi7vq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1VLarD8OOdyldc9y6GKgycuSOT751rQ0O\" width=800></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9XMSnsBmh4Qi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly, option 2 is better option as\n",
        "- it is in the direction of the data\n",
        "- and preserves more variance."
      ],
      "metadata": {
        "id": "WzBl4eUqivG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 2nd PC will lie perpendicular to first one.\n",
        "- However, we are not interested in it.\n",
        "\n"
      ],
      "metadata": {
        "id": "ijCaH4wYjiB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What will data look like after projection ?\n"
      ],
      "metadata": {
        "id": "XgjXr-x_jyUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once, we project the data on to PC1.\n",
        "\n",
        "The data will look like:"
      ],
      "metadata": {
        "id": "qaobmv-TjuGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1UDYx_dc4olKWLGKpJPhuiXovqndKipWi\" width=800></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SEutoUzljvmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that\n",
        "- how there is a loss in information\n",
        "- which eventually causes drop in accuracy."
      ],
      "metadata": {
        "id": "0vGhbyutlK06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Geometrical objective"
      ],
      "metadata": {
        "id": "pxWeP-_zlR3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We saw geometrically\n",
        "- which Principal component will be preseve max variance.\n",
        "\n",
        "#### But, how do we find it mathematically?"
      ],
      "metadata": {
        "id": "tnC9-ODU-6TR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the following PCs, how do we say mathematically, which one is preserving max information ?"
      ],
      "metadata": {
        "id": "3kIiRGfd_Mmk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1SUGVrK57GJL1OTCpqzGVV0-9OsnOTdPN\" width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "xnjNAo6k_WG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to come up with a loss/ cost function\n",
        "- which helps in finding the best PC.\n",
        "\n",
        "Any ideas ?"
      ],
      "metadata": {
        "id": "0AlmsR7iANSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### First idea"
      ],
      "metadata": {
        "id": "EwfwTfUgAcTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The perpendicular distance of points from the best PC will be minimum.**\n",
        "\n",
        "Let's check it out visually."
      ],
      "metadata": {
        "id": "1h0-vYtHBYqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1CFnv-9p6fqgYZpRVm1EoURvfRtAky6Oa\" width=800></center>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FUTNu9b_DPRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that\n",
        "- distances of point from w2 are large compared to that of w1.\n",
        "\n",
        "\n",
        "This shows that the best PC will have minimal distances from datapoints.\n",
        "\n",
        "\n",
        "There's another idea for finding best PC"
      ],
      "metadata": {
        "id": "2iQOqogaGxHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second idea"
      ],
      "metadata": {
        "id": "SlIqsZPuHDDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that,\n",
        "- each datapoint is a vector and it is represented as :"
      ],
      "metadata": {
        "id": "dM1QUQ-UHVDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=19BLvW8WQYALoQcatqO7n5PxCwOStLCdl\" width=800></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jaLbK9zjnae8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, recall calulating the projection of a vector\n",
        "\n",
        "Suppose, we are given a vector $x_1$ and we want to find its projection length on vector $f_1$, it'll be as follows:"
      ],
      "metadata": {
        "id": "HRRc5WBBFunx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1Ois0rIzTWl3MHJfYcadC5kGa3Effdrpk\" width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "dyfs5LOWF_D9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now,\n",
        "\n",
        "the first idea was to\n",
        "- find the vector which minimizes the $\\perp$ distance\n",
        "\n",
        "We can say that,\n",
        "- Minimizing perpendicular distance is same as maximizing projection length of datapoints on the PC.\n",
        "\n"
      ],
      "metadata": {
        "id": "VzaXMTOmIF3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### But, why minimizing perpendicular distance is same as maximizing projection length ?"
      ],
      "metadata": {
        "id": "A124VpIqPEY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Say, we have a datapoint and two PC: a and b.\n",
        "\n",
        "datapoint is closer to vector a\n",
        "- so it'll have smaller perpendicular distance\n",
        "\n",
        "while vector b is farther from it\n",
        "- perpendicular distance from datapoint to vector will be large.\n",
        "\n",
        "If we were to follow basic principle of right angle triangle,\n",
        "- as the hypotenuse is fixed\n",
        "    - if height becomes large, then\n",
        "    - base of the triangle will become small in order to keep hypotenuse length fixed.\n",
        "\n",
        "Hence, smaller the perpendicular length,\n",
        "- larger the projection length."
      ],
      "metadata": {
        "id": "tgIKNnFdL9Jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=14IhTzYJjpqzjifZmpv7y1sdM-JU0TXvR\" width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "0hcoo7zJItAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Maximizing projection is same as maximizing variance"
      ],
      "metadata": {
        "id": "4vdH2hqCPKi4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, note that:\n",
        "- having maximum projection length is similar to having feature with maximum variance.\n",
        "\n",
        "\n",
        "If the new feature is in the direction of data,\n",
        "- it'll have max variance/ speard of data\n",
        "- and also maximum projection length\n",
        "\n",
        "\n",
        "So, if were to find feature which has maximum projection length\n",
        "- we are indirectly finding the feature which is preserving maximum variance"
      ],
      "metadata": {
        "id": "MOZX4sgZL5PP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look it through an example:"
      ],
      "metadata": {
        "id": "VmnnhLvVNspn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1OEN9dn1VY_NQm9tDgDlt11YaKeduKn8C\" width=800></center>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M61g2ToDOsiY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "and if we were to look at previous diabetes example:"
      ],
      "metadata": {
        "id": "LBbzkNzjPBYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=15YiNGRhEfh7QjV2CCfeVhgUiKd-1NQ_d\" width=800></center>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i6ZmKqQ4PUMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA is all about finding the components which maximizes my variance preservation.\n",
        "- which geometrically is same as maximizing projection lengh\n",
        "- or same as minimizing perpendicular length."
      ],
      "metadata": {
        "id": "l-yBfj4iPmCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualization"
      ],
      "metadata": {
        "id": "Smz5bCJxRfXA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an intersting visualziation that shows how spread and perpendicular length changes as the Principal component changes"
      ],
      "metadata": {
        "id": "V2gowU5CPqZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1FGSBn8Y-F8bs88WonxRJF98jnKBpO6kp\" width=1000></center>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BtdzNDXRRe14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the length of the spread\n",
        "- as the perpendicular length increases and decreases."
      ],
      "metadata": {
        "id": "aSeWCi_iRkFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extra read: Intuitive example for PCA"
      ],
      "metadata": {
        "id": "-3eJ6xU5RywD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579"
      ],
      "metadata": {
        "id": "TLXnmMF6R3nt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mathematical Proof - Not important"
      ],
      "metadata": {
        "id": "lj7t3eHYR55-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting off with the mathematical proof\n",
        "\n",
        "#### Do we need to scale the data?"
      ],
      "metadata": {
        "id": "y9sSQx1Lg5jM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since, we'll be dealing with features which are in different scales\n",
        "\n",
        "> age in years, height in feet, and weight in kgs.\n",
        "\n",
        "So, in order to bring to features to same scale,\n",
        "- we need to scale the data first.\n",
        "\n",
        "Another reason is:\n",
        "- PCA is a **linear transformation technique** that seeks to find linear combinations of variables that capture the most variance.\n",
        "- Without scaling, PCA may give more weight to variables with larger scales,\n",
        "    - assuming that they have higher importance or variance, which may not be true."
      ],
      "metadata": {
        "id": "AwvHl-9FhG1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Which scaling to use ?"
      ],
      "metadata": {
        "id": "RteLoHoti5cz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use Standardization (which involves mean centering) for scaling the data."
      ],
      "metadata": {
        "id": "0QwojHtai7Fb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why?**"
      ],
      "metadata": {
        "id": "FBTsrGZyjSrP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are few reasons:\n",
        "- To make it robust to outliers\n",
        "\n",
        "- When we normalize data using min-max scaling,\n",
        "    - we are scaling each feature to a specified range (e.g., [0, 1]).\n",
        "    - This can change the relative importance of features if they originally had different ranges or units.\n",
        "    \n",
        "For example, if one feature originally had a range of [0, 100] and another feature had a range of [0, 1],\n",
        "- the first feature would have a broader range and might contribute more to the variance.\n",
        "\n",
        "**In short, it skews the distribution.**\n"
      ],
      "metadata": {
        "id": "KuWL26V0jT7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the other hand, in standardization :\n",
        "- Mean centering makes sure\n",
        "    - all features contribute equally to the first principal component\n",
        "    - because they are all centered at the same point (zero) in the new coordinate system.\n",
        "\n",
        "- This scaling step ensures that the variance of each feature in the data becomes equal to 1.\n",
        "    - which gives each feature an equal weighting in terms of variability.\n",
        "\n",
        "**What does it mean?**\n",
        "\n",
        "- Features with larger standard deviations will have a larger spread of values,\n",
        "- but they are scaled down so that their influence on the principal components is proportional to their variability."
      ],
      "metadata": {
        "id": "HCsTID7mkeKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data after scaling will look like:"
      ],
      "metadata": {
        "id": "j1yIq4LunloT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1TQTc0kqEgRF7GZeoNQENqlk9tl8ZN8_Q\" width=800></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Au8oiwAjpjOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the translation part of PCA."
      ],
      "metadata": {
        "id": "tqh8V6Xmqcd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Assumptions"
      ],
      "metadata": {
        "id": "zV96dVt-q8we"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assume that the principal component passes through origin\n",
        "\n",
        "**Why?**\n",
        "\n",
        "As we are only interested in the direction of the PC,\n",
        "- we don't care about infinite parallel lines or intercept term.\n",
        "- also makes the derivation easier."
      ],
      "metadata": {
        "id": "5gAsbaVEq-sm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=16rTTIy4KHMNZ8vUzT4ctqoV4TtI2go0W\" width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "yIyDLCoCrCXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Objective function"
      ],
      "metadata": {
        "id": "MKneIJkWrMma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Goal**: We want to maximize the length of the projection."
      ],
      "metadata": {
        "id": "OFGSYYCes-Jc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, when we calculate the vectors projection on u,\n",
        "\n",
        " the objective function comes out as:"
      ],
      "metadata": {
        "id": "rvuElNjHv_XR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1rZIYPToUBgSpXsCaKM0v-BtoFQwGASso\" width=800></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9csyI2Oxt6xZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, instead of maximizing sum of length of projection\n",
        "- we'll maximize avg sum of length of projection\n",
        "\n",
        "**Why?**\n",
        "- as the number of datapoints inc, the sum will increase\n",
        "- so as to make it make the sum agnostic of count of datapoints."
      ],
      "metadata": {
        "id": "p_kXvc9w5KfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As stated earlier that we are only interested in the direction of $\\vec{u}$\n",
        "\n",
        "To make calculation simpler,\n",
        "- we assume it to be a **unit vector**"
      ],
      "metadata": {
        "id": "Pk060KLxv7M4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1IVLQz2RWmLHSm9MKnbM9fVi7_80KoAoi\" width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "F1XBPLdKwWGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "- This is a constraint optimization\n",
        "\n",
        "We'll convert it to unconstraint optimization using **Lagrange Multiplier**"
      ],
      "metadata": {
        "id": "PHXif5gRx6Cx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1-0LD53MZJ-JbnSogjQNeGmbef_6J683u\" width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "-ffsX-0byHon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### But, projection length can also be negative. How do we handle that?"
      ],
      "metadata": {
        "id": "EaS8QYqGzYzp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order for projection avoid cancelling out,\n",
        "- we can take modulus of the equation.\n",
        "\n",
        "\n",
        "But, equation won't be differentiable if we take mod.\n",
        "\n",
        "**How do we handle this?**\n",
        "\n",
        "We can take a square of $x_i. \\vec{u}$."
      ],
      "metadata": {
        "id": "rmHmaRzNzmNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=141IGEs_APrWB6ruj-UPA5FRgnWDb4mW6\" width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "Rh338RP3ztP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that,\n",
        "- we are summating the square of dot product of two vectors i.e. $x_i$ and $\\vec{u}$\n",
        "\n",
        "Instead, we can simply take dot product of whole X matrix with $\\vec{u}$\n",
        "- This removes the summation."
      ],
      "metadata": {
        "id": "KL0r0rOE0uOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1w3pSAwpHmWQlCUyK90bPqwK62psmlQcs\" width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "jexI5KHs1KGH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For convinience,\n",
        "- we'll take a square of $||u⃗||$ as well\n",
        "\n",
        "As it is a unit vector,\n",
        "- it won't have any impact."
      ],
      "metadata": {
        "id": "tOnptJbB2S1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1yxKozsOVC1jPrvlg0tH6O4kee8ID3kFo\" width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "Vtq0vFyA2kIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1K68yObaazgdDarkjm3_IiuF3jJmPTkkA\" width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "Ob0X6BUG3k3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that:\n",
        "- V is called the **covariance matrix**"
      ],
      "metadata": {
        "id": "K4pBGpz07DYZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are performing $X^T.X$.\n",
        "\n",
        "The multiplication will look as follows:"
      ],
      "metadata": {
        "id": "NwePzCTl7Z46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1sqFVEBAMtTZc7JOGEDjCtJJDJrVdHvId\" width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "xhDdvm__JvpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we were to compute the first multiplication, it would come out to be:"
      ],
      "metadata": {
        "id": "SvvIvySdLHCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1ZO5ixqRd1u0dRvxKl99Lnl1XuFGtosv7\" width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "EczvJHIfLNnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1ouydwpdovvRiS_wxPbEBPkmu6zDLkggQ\" width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "9eVcw4dmMFGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, all the diagonal elements of the resultant matrix will hold the values of respective feature\n",
        "\n",
        "And all the non diagonal features will contain value of covariance between feature."
      ],
      "metadata": {
        "id": "czSRqipcMcU4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solving the optimization problem"
      ],
      "metadata": {
        "id": "9sF4d4mdMv_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to solve the optimization problem,\n",
        "- we will have calculate partial graidents."
      ],
      "metadata": {
        "id": "y9I2zXTDM8_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1oRsxUM2-6IdMX64eT1QNvLcx_9fM-OlP\" width=800></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OUVXw_cGNRHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1i3_xEbsRv453ikDdxznfwyKqOd4hL8i1\" width=800></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hH_jQxfiPlzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1KjgMne4IR61obXhQ7W5C0JYNkGkVQu1v\" width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "gAHh825uPqYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that:\n",
        "- V here is a matrix\n",
        "- Vector u must be such that when we multiply V with it,\n",
        "    - it is same as multiplying a scalar with it.\n",
        "\n",
        "which is what makes u a special vector.\n",
        "\n",
        "Let's see what's so special about it."
      ],
      "metadata": {
        "id": "_FvbyhQWPxTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eigen values and vectors"
      ],
      "metadata": {
        "id": "hwa3sjB_QZ5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say we have a vector u and V such that:"
      ],
      "metadata": {
        "id": "VTJKRUmaQkV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1u5WW9xrV75CLeUqjdWU1xzmTl5sh-nNm\" width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "39LYs2NWRTBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we multiply u with V, we get:"
      ],
      "metadata": {
        "id": "UAW6DjruSY33"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1Dj5nG0Xzw1tJoa59efcdtIYP5bFJuZkD\" width=800></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7FVu_iFtSd9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is what happens when typically,\n",
        "- when we multiply a matrix to a vector\n",
        "\n",
        "It changes its magnitude and direction.\n",
        "- i.e. it'll strech and rotate the vector"
      ],
      "metadata": {
        "id": "wTtYce_vTfle"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But every matrix has a special vector s.t.\n",
        "- when we multiply them,\n",
        "    - it'll only stretch the vector but won't change its direction.\n",
        "\n",
        "\n",
        "Let's see how it does that:"
      ],
      "metadata": {
        "id": "1fd-TCYaUCzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1OzW5Cu0C8dXvi6UvrowEwpo0WNsvk0RH\" width=800></center>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2moufDKrUNWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This special vector which doesn't change the direction which when multiplied by matrix is called **eigen vector**\n",
        "\n",
        "So, u is eigen vector of V\n",
        "- it is tied to the matrix."
      ],
      "metadata": {
        "id": "3JWod1N4VKnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "- If we have 2x2 matrix, we'll have 2 eigen vector\n",
        "- or if we have 7x7 matrix, we'll have 7 eigen vectors"
      ],
      "metadata": {
        "id": "-CSdWB4oVSad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the multiple by which the magnitude changes\n",
        "- is called **eigen value**\n",
        "\n",
        "So, here, 6 is the eigen value."
      ],
      "metadata": {
        "id": "lqu_Jdt_Vh1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### To summarize:\n"
      ],
      "metadata": {
        "id": "S75sAz7xVqzp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1GFxx6UlVmQipFsUmhiVxApWp18h2o1ww\" width=800></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bHOMjZS-Wbfg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's a special vector (eigen vector) which when multiplied with a matrix (V)\n",
        "- gives the same eigen vector multipled a eigen value."
      ],
      "metadata": {
        "id": "bwqxskLFWHc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If V is $mxm$  matrix\n",
        "- then we'll have m eigen vector\n",
        "    - each eigen vector will have its own eigen value."
      ],
      "metadata": {
        "id": "vT6ekAFvWTjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coming back to the condition we got:"
      ],
      "metadata": {
        "id": "X632fSIZWqoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1q-NPRmSWZZTBRaMUOTPGluu8FOBfo6KX\" width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "hZgjw9hjXCiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we need to find you,\n",
        "- we just need to find all the eigen vectors of V\n",
        "- i.e. all the eigen vectors of cov. mat (A)"
      ],
      "metadata": {
        "id": "idb7-cFPXLQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interpreting  Eigen vector and values"
      ],
      "metadata": {
        "id": "X-gu6WMeYE96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Say, we have 10 features\n",
        "\n",
        "So, we'll have 10 eigen vectors and 10 eigen values"
      ],
      "metadata": {
        "id": "dpS7qt5xYVAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An interesting property is:\n",
        "- Largest eigen value corresponds to most important eigen vector\n",
        "    - i.e. the vector which preserves maximum variance\n",
        "- where as smallest eigen value corresponds to the least important eigen vector\n",
        "    - i.e. vector which preserves minimum variance.\n",
        "\n",
        "\n",
        "So, eigen value tells the importance of eigen vectors"
      ],
      "metadata": {
        "id": "D5-rjxJSYeQA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sRjxIc35YzWz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}